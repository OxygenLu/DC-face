{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "import os.path as osp\n",
    "import os, sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "from lib.utils import load_model_weights,mkdir_p\n",
    "from models.GALIP import NetG, CLIP_TXT_ENCODER\n",
    "import argparse\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from basicsr.utils import imwrite\n",
    "from gfpgan import GFPGANer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # 'cpu' # 'cuda:0'\n",
    "CLIP_text = \"ViT-B/32\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model = clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIP_TXT_ENCODER(clip_model).to(device)\n",
    "netG = NetG(64, 100, 512, 256, 3, False, clip_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIP_TXT_ENCODER(clip_model).to(device)\n",
    "netG = NetG(64, 100, 512, 256, 3, False, clip_model).to(device)\n",
    "path = '../saved_models/state_epoch_760.pth'\n",
    "# path = '/home/luxj/document/GALIP-main/code/saved_models/faces/GALIP_nf64_normal_faces_256_2024_03_03_17_39_53/state_epoch_620.pth'\n",
    "checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "netG = load_model_weights(netG, checkpoint['model']['netG'], multi_gpus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 #需要生成多少张图片\n",
    "noise = torch.randn((batch_size, 100)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入语料\n",
    "captions = ['He is young, and slim with high cheekbones, quite rounded nose, an oval face, no bangs, straight eyebrows, quite small lips. He has straight blond hair. He has a normal chin, no beard.  He doesn\\'t put neither lipstick, neck tie, necklace, earrings, heavy makeup, hat nor eyeglasses.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate from text\n",
    "# with torch.no_grad():\n",
    "#     for i in range(len(captions)):\n",
    "#         caption = captions[i]\n",
    "#         tokenized_text = clip.tokenize([caption]).to(device)\n",
    "#         sent_emb, word_emb = text_encoder(tokenized_text)\n",
    "#         sent_emb = sent_emb.repeat(batch_size,1)\n",
    "#         fake_imgs = netG(noise,sent_emb,eval=True).float()\n",
    "#         # name = f'{captions[i].replace(\" \", \"-\")}'\n",
    "#         name = 'demo4'\n",
    "#         vutils.save_image(fake_imgs.data, './samples/%s.png'%(name), nrow=8, value_range=(-1, 1), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(len(captions)):\n",
    "        caption = captions[i]\n",
    "        tokenized_text = clip.tokenize([caption]).to(device)\n",
    "        sent_emb, word_emb = text_encoder(tokenized_text)\n",
    "        sent_emb = sent_emb.repeat(batch_size, 1)\n",
    "        fake_imgs = netG(noise, sent_emb, eval=True).float()\n",
    "        # 分开保存\n",
    "        for j in range(batch_size):\n",
    "            fake_img = fake_imgs[j]\n",
    "            name = 'demo_%d' % (i * batch_size + j + 1)\n",
    "            vutils.save_image(fake_img, './samples/%s.png' % (name), value_range=(-1, 1), normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python inference_gfpgan.py -i inputs/faces -o results -v 1.3 -s 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args setting\n",
    "input = \"./samples\"\n",
    "mkdir_p('./results')\n",
    "output = './results'\n",
    "bg_upsampler = 'realesrgan'\n",
    "tile = 400\n",
    "version = 1.3 #默认版本1.3 ，共有1 | 1.2 | 1.3\n",
    "upscale = 2 \n",
    "weight = 0.5\n",
    "suffix = None\n",
    "ext = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ input & output ------------------------\n",
    "if input.endswith('/'):\n",
    "    input = input[:-1]\n",
    "if os.path.isfile(input):\n",
    "    img_list = [input]\n",
    "else:\n",
    "    img_list = sorted(glob.glob(os.path.join(input, '*')))\n",
    "# os.makedirs(output, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ------------------------ set up background upsampler ------------------------\n",
    "if bg_upsampler == 'realesrgan':\n",
    "    if not torch.cuda.is_available():  # CPU\n",
    "        import warnings\n",
    "        warnings.warn('The unoptimized RealESRGAN is slow on CPU. We do not use it. '\n",
    "                        'If you really want to use it, please modify the corresponding codes.')\n",
    "        bg_upsampler = None\n",
    "    else:\n",
    "        from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "        from realesrgan import RealESRGANer\n",
    "        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n",
    "        bg_upsampler = RealESRGANer(\n",
    "            scale=2,\n",
    "            model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n",
    "            model=model,\n",
    "            tile=tile, # 超参数默认400\n",
    "            tile_pad=10,\n",
    "            pre_pad=0,\n",
    "            half=True)  # need to set False in CPU mode\n",
    "else:\n",
    "    bg_upsampler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luxijun/anaconda3/envs/yolo/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/luxijun/anaconda3/envs/yolo/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ set up GFPGAN restorer ------------------------\n",
    "arch = 'clean'\n",
    "channel_multiplier = 2\n",
    "# model_name = 'GFPGANv1.3'\n",
    "model_path = './pt/GFPGANv1.3.pth'\n",
    "\n",
    "restorer = GFPGANer(\n",
    "    model_path=model_path,\n",
    "    upscale= upscale,\n",
    "    arch=arch,\n",
    "    channel_multiplier=channel_multiplier,\n",
    "    bg_upsampler=bg_upsampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing demo_1.png ...\n",
      "Processing demo_2.png ...\n",
      "Processing demo_3.png ...\n",
      "Processing demo_4.png ...\n",
      "Processing demo_5.png ...\n",
      "Processing demo_6.png ...\n",
      "Processing demo_7.png ...\n",
      "Processing demo_8.png ...\n",
      "Results are in the [./results] folder.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ restore ------------------------\n",
    "\n",
    "for img_path in img_list:\n",
    "    # read image\n",
    "    img_name = os.path.basename(img_path)\n",
    "    print(f'Processing {img_name} ...')\n",
    "    basename, ext = os.path.splitext(img_name)\n",
    "    input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # restore faces and background if necessary\n",
    "    cropped_faces, restored_faces, restored_img = restorer.enhance(\n",
    "        input_img,\n",
    "        has_aligned='store_true',\n",
    "        only_center_face='store_true',\n",
    "        paste_back=True,\n",
    "        weight=weight)\n",
    "\n",
    "    # save faces\n",
    "    for idx, (cropped_face, restored_face) in enumerate(zip(cropped_faces, restored_faces)):\n",
    "        # save cropped face\n",
    "        # save_crop_path = os.path.join(output, 'cropped_faces', f'{basename}_{idx:02d}.png')\n",
    "        # imwrite(cropped_face, save_crop_path)\n",
    "\n",
    "        # save restored face\n",
    "        if suffix is not None:\n",
    "            save_face_name = f'{basename}_{idx:02d}_{suffix}.png'\n",
    "        else:\n",
    "            save_face_name = f'{basename}_{idx:02d}.png'\n",
    "        save_restore_path = os.path.join(output, 'restored_faces', save_face_name)\n",
    "        imwrite(restored_face, save_restore_path)\n",
    "\n",
    "        # save comparison image\n",
    "        cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n",
    "        imwrite(cmp_img, os.path.join(output, 'cmp', f'{basename}_{idx:02d}.png'))\n",
    "\n",
    "    # save restored img\n",
    "    if restored_img is not None:\n",
    "        if ext == 'auto':\n",
    "            extension = ext[1:]\n",
    "        else:\n",
    "            extension = ext\n",
    "\n",
    "        if suffix is not None:\n",
    "            save_restore_path = os.path.join(output, 'restored_imgs', f'{basename}_{suffix}.{extension}')\n",
    "        else:\n",
    "            save_restore_path = os.path.join(output, 'restored_imgs', f'{basename}.{extension}')\n",
    "        imwrite(restored_img, save_restore_path)\n",
    "\n",
    "print(f'Results are in the [{output}] folder.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "849434eb86c3997df801551b732438d01b491303b69c29efcf332721ce6d8430"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
